## Bibliography

### Related work

- Sorting-related experiments with deep learning [A], [B].
- Relational reasoning [C], [D].
- Semantic Relations in sequential representations (word embeddings) [E], [F].
- Convolutional Neural Networks for Natural Language Processing [G], [H], [I].
- Links to information theory and deep learning [X], [Y].
- Bottleneck principle [Z], [W].

There are some other works where simple algorithms are inferred via neural networks. For example, in [A], the operations of copying and sorting an array are performed with a fully differentiable network connected to an eternal memory via attention.

In another approach, [B] presents a sorting experiment for sets of numbers using a sequence to sequence model. The fundamental question which the study raises is how to represent sets as a sequence when the underlying order is not known or doesn't exist.

The importance of prior information and pre-learned intermediate concepts. Composition of 2 highly non-linear tasks and other hypothesis such as local minima obstacle and guided/transfer learning [X].

Gradients in highly composed functions or hard constraints. Accuracy as a function of input dimensionality. Gradient based methods cannot learn reasonably fast random parities and linear-periodic functions [Y].

### References

#### Core deep learning models

[A] A. Graves, G. Wayne, and I. Danihelka, “Neural Turing Machines,” arXiv:1410.5401v2, 2014.

[B] O. Vinyals, S. Bengio, and M. Kudlur, “OrderMatters: Sequence to Sequence for Sets”, in 4th International Conference on Learning Representations (ICLR), 2016.

[C] A. Santoro et al, "A simple neural network module for relational reasoning"

[D] D. Hudson and C. Manning, "Compositional Attention Networks for Machine Reasoning"

[E] Distributed Representations of Words and Phrases and their Compositionally, T. Mikolov, 2013

[F] Distributed Representations of Sentences and Documents, Q. Le, T. Mikolov, 2014

[G] Convolutional Neural Networks for Sentence Classification, Y. Kim, 2014

[H] Convolutional Neural Networks for Modelling Sentences, N. Kalchbrenner,
2014

[I] Deep Learning applied to Natural Language Processing, M-M. Lopez, J.
Kalita, 2017


#### Misc

[J] I. Kant, Critique of Pure Reason

[K]

[L] R. Sutton, The Bitter Lesson

[M] Matlab, DAG, url = https://www.mathworks.com/help/deeplearning/ref/dagnetwork.html

[N] Tensorflow, Word2Vec, url = https://www.tensorflow.org/tutorials/word2vec

[O]

[P] M. Patrascu, Problema SortNet, Infoarena, url = https://infoarena.ro/problema/sortnet

[Q]

[R]

[S] 

[T]

[U]

[V]

#### Deep learning theory

[X] C. Gulcehre and Y. Bengio, "Knowledge Matters: Importance of Prior Information for Optimization", Journal of Machine Learning Research, 2016.

[Y] S. Shalev-Shwartz and O. Shamir and S. Shammah, "Failures of Gradient-Based Deep Learning", arXiv:1703.07950, 2017.

[Z] Ravid Schwartz-Ziv and Naftali Tishby, "Opening the black box of Deep Neural Networks via Information"

[W] Naftali Tishby and Noga Zaslavsky "Deep Learning and Information Bottleneck Principle"

#### My own and joint work (published and unpublished)

[1] Finding Patterns in Visualization of Programs, conference paper, PPIG 2017

[2] Using the Self-Organizing Map to cluster Computer Programs based on Spatio-Temporal Features extracted from Execution Traces, unpublished, 4 versions presented for Masters & Research reports

[3] Detection and Emulation of Algorithms from Execution Traces, under review KNOSYS Elsevier journal

[4] Hierarchical segmentation of graphical interfacesf or Document Object Model reconstruction, workshop paper, ICML 2018

[5] Modeling cognitive processes underlying computer programming, conference paper, PPIG 2018